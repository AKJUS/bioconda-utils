version: '3.7'

volumes:
  rabbitmq-data:  # persists items in the queue
  traefik-certs:  # persists SSL certs to avoid re-issuing on restart
  esdata:

networks:
  backend:
  logging:

secrets:
  app_id:
    file: secrets/app_id.txt
  app_key:
    file: secrets/app_key.pem
  app_secret:
    file: secrets/app_secret.txt
  code_sign_key:
    file: secrets/code_sign_key.asc
  erlang_cookie:
    file: secrets/erlang_cookie.txt

# common parameters for web and worker services using the
# bioconda-utils code base
x-app-common: &app-common
  image: epruesse/biocondabot:latest
  networks:
    - backend
  environment:
    # make sure python doesn't try to buffer stdout/stderr
    # (we want to see all log messages right away)
    - PYTHONUNBUFFERED=true
    # Path to secret key for signing JWTs to authenticate as Github App
    - APP_KEY_FILE=/run/secrets/app_key
    # Path to github App ID (not actually secret)
    - APP_ID_FILE=/run/secrets/app_id
    # Path to secret for SHA signing of incoming webhooks
    - APP_SECRET_FILE=/run/secrets/app_secret
    # Path to GPG key for signing commits
    # (This key is registered with a bot account, not the app,
    #  as github apps have no email they can't have a commit signing key)
    - CODE_SIGNING_KEY_FILE=/run/secrets/code_sign_key
    # Points to the AMQP message queue server
    - CLOUDAMQP_URL=amqp://mq:5672
  secrets:
    # See above
    - app_key
    - app_id
    - app_secret
    - code_sign_key
  depends_on:
    - mq

services:
  ## Message Queue
  mq:
    hostname: mq    
    image: rabbitmq:latest
    secrets:
      - source: erlang_cookie
        target: /var/lib/rabbitmq/.erlang.cookie
        uid: '999'
        gid: '999'
        mode: 0600
    user: rabbitmq
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
    networks:
      - backend
    deploy:
      resources:
        limits:
          memory: 200M

  ## Bioconda-Utils web app
  web:
    <<: *app-common
    hostname: web
    build:
      context: ..
      dockerfile: Dockerfile.web
    command: >-
       gunicorn 
       --worker-class aiohttp.worker.GunicornWebWorker
       --bind 0.0.0.0:80
       bioconda_utils.bot.web:start
       #(disable_internal_celery=True)
    depends_on:
      - mq
    deploy:
      labels:
        # Export this on Traefik to outside
        traefik.enable: "true"
        # This container runs on port 80
        traefik.port: 80
        # And should be mapped to https://biocondabot.pruesse.net
        traefik.frontend.rule: "HostRegexp: {subdomain:biocondabot.*}.pruesse.net,localhost"
      resources:
        limits:
          memory: 200M

  ## Bioconda-Utils workers handling tasks scheduled from web app
  worker:
    <<: *app-common
    hostname: worker
    command: >-
      celery worker
      -l info
      -A bioconda_utils.bot.worker
      --without-heartbeat
    deploy:
      mode: replicated
      replicas: 1  # let's have 2 for now
      resources:
        limits:
          memory: 700M

  ## Check services for updates to containers
  shepherd:
    hostname: shepherd
    image: mazzolino/shepherd:0.3.0
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - SLEEP_TIME=5m  # check every 5 minutes
      - BLACKLIST_SERVICES="shepherd"  # things not to check
    deploy:
      placement:
        constraints:
        - node.role == manager  # must run on manager node
      resources:
        limits:
          memory: 20M

  ## Celery monitoring tool
  # We map this to /flower using traefik
  flower:
    image: mher/flower
    hostname: flower
    command: >-
      flower
      --broker=amqp://mq:5672
      --port=8080
      --url_prefix=flower
    networks:
      - backend
    depends_on:
      - mq
    deploy:
      labels:
        traefik.enable: "true"
        traefik.port: 8080
        traefik.frontend.rule: "Host: biocondabot.pruesse.net,localhost; PathPrefixStrip: /flower"
      resources:
        limits:
          memory: 100M

  ## Traefik frontent gateway
  # - Handles SSL encryption
  # - Maps outside domain/path to inside services
  # Note: Traefik can't do HTTP content rewriting for url rewrites.
  #       E.g. flower needs to be told that it will be exposed as http://host/flower/...
  #       rather than http://host/..., so that it can point links to resources (css, images)
  #       to the right place.
  frontend:
    image: traefik
    hostname: traefik
    command: >-
      --api
      --docker
      --docker.swarmMode
      --docker.exposedByDefault=false
      --docker.watch
      --docker.domain=biocondabot.pruesse.net
      --entryPoints="Name:http Address::80 Redirect.EntryPoint:https"
      --entryPoints="Name:https Address::443 TLS"
      --defaultentrypoints=http,https
      --logLevel=INFO
      --acme
      --acme.entryPoint=https
      --acme.httpChallenge
      --acme.httpChallenge.entrypoint=http
      --acme.domains=biocondabot.pruesse.net
      --acme.email=info@pruesse.net
      --acme.storage=acme/certs.json
      --acme.acmelogging
      --accessLog
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /dev/null:/traefik.toml
      - traefik-certs:/acme
    networks:
      - backend
    deploy:
      labels:
        traefik.port: 8080
        traefik.frontend.rule: Host:biocondabot.pruesse.net,localhost;PathPrefixStrip:/traefik/
      resources:
        limits:
          memory: 50M


## ELK Stack - needs a TON of memory (like 8GB, definitely more than the ec2 has)
#  elasticsearch:
#    image: docker.elastic.co/elasticsearch/elasticsearch:6.7.0
#    hostname: elasticsearch
#    environment:
#      - "ES_JAVA_OPTS=-Xms128m -Xmx128m"
#    command:
#      - elasticsearch
#    volumes:
#      - esdata:/usr/share/elasticsearch/data
#    networks:
#      - logging
#    deploy:
#      replicas: 1
#
#  logstash:
#    image: docker.elastic.co/logstash/logstash:6.7.0
#    hostname: logstash
#    depends_on:
#      - elasticsearch
#    environment:
#      - "LS_JAVA_OPTS=-Xms128m -Xmx128m"
#    command: >-
#      logstash -e '
#        input { udp { port => 5000 codec => json } }
#        filter { if [docker][hostname] =~ /^logstash/ { drop { } } }
#        output { stdout { } elasticsearch { hosts => "elasticsearch:9200" } }
#        '
#    networks:
#      - logging
#    deploy:
#      replicas: 1
#
#  kibana:
#    image: docker.elastic.co/kibana/kibana:6.7.0
#    hostname: kibana
#    #image: bitnami/kibana
#    networks:
#      - logging
#      - backend
#    environment:
#        - SERVER_REWRITEBASEPATH=false
#        - SERVER_BASEPATH=/kibana
#        - ELASTICSEARCH_URL=http://elasticsearch:9200
#    depends_on:
#      - elasticsearch
#    deploy:
#      labels:
#        traefik.enable: "true"
#        traefik.port: 5601
#        traefik.frontend.rule: Host:biocondabot.pruesse.net,localhost;PathPrefixStrip:/kibana
#      replicas: 1
#
#  logspout:
#    image: bekt/logspout-logstash
#    hostname: logspout
#    environment:
#      ROUTE_URIS: logstash://logstash:5000
#    volumes:
#      - /var/run/docker.sock:/var/run/docker.sock
#    networks:
#      - logging
#    depends_on:
#      - logstash
